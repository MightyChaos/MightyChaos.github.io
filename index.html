<!DOCTYPE html>
<html>

<head>
<title>Wang Chaoyang's Homepage</title>
<meta name="author" content="Wang Chaoyang">
<meta name="description" content="Wang Chaoyang's Homepage">
<meta name="keywords" content="Chaoyang Wang, Wang Chaoyang, 4real, 4real-video, 4d generaion, 4d reconstruction, tracking, deep-LK, binocular, photometric, stereo, multi-branched hierarchical segmentation, unsupervised learning depth">
<style type="text/css">
p {font-size:14px;font-family:arial;color:black;}
li {font-size:14px;font-family:arial;color:black;}
h1 {font-size:30px;font-family:arial;color:gray;}
h2 {font-size:25px;font-family:arial;color:gray;}
h3 {font-size:14px;font-family:arial;color:black;margin-top:0;margin-bottom:0}
a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

papertitle {
	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	font-size: 15px;
	font-weight: 700
}

name {
	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	font-size: 32px;
}

</style>
</head>

<center>
<body leftmargin = "50">
<center>
<table border="0">
	<tr>
		<td width="250">
		<img src="night_king3.png" alt="Smiley face" width="200">
		</td>
		<td width="500">
		<table border="0">
			<tr>
			<h1>Chaoyang Wang</h1>
			</tr>
			<tr>
				<!-- <ul style="list-style-type:none"> -->
					<p>I am a research scientist at <a href="https://research.snap.com/team/category/creative-vision.html">Snap Creative Vision Team</a>, leading research in 4D reconstruction and generation.</p>
					<p>
					I obtained my PhD degree from Robotics Institute of CMU, and was a member of <a href="http://www.ci2cv.net/">CI2CV Lab</a> led by Prof. Simon Lucey. Prior to CMU, I obtained my B.E. and M.S. degree from Shanghai Jiao Tong University.
					<!-- Prior to CMU, I obtained my B.E. and M.S. degree from Shanghai Jiao Tong University,  supervised by <a href="https://scholar.google.com/citations?user=1smFmxAAAAAJ&hl=en">Prof. Liqing Zhang</a>.
					I've collaborated with <a href="https://oraziogallo.github.io/">Orazio Gallo</a>, Ben Eckart at Nvidia, <a href="http://www.oliverwang.info/">Oliver Wang</a> and <a href="https://fperazzi.github.io/">Federico Perazzi</a> at Adobe Inc.
						<a href="https://yichenwei.github.io">Yichen Wei</a>, <a href="https://www.microsoft.com/en-us/research/people/lijuanw/">Lijuan Wang</a>,
						<a href="http://www-infobiz.ist.osaka-u.ac.jp/en/member/matsushita/">Yasuyuki Matsushita</a>, 
						<a href="https://www.microsoft.com/en-us/research/people/frankkps/">Frank K. Soong</a> at Microsoft Research Asia. -->
					</p>
					<p>If you are interested in collaboration or looking for research internship position, feel free to shoot me an email at cwang9[at]snapchat.com</p>
					<!-- <li><strong><a href="cv.pdf">resume</a></strong></li> -->
				<!-- </ul> -->
			</tr>
		</table>
		</td>
	</tr>
</table>
</center>

<!-- <hr>
<h2>Recent Works</h2>
<table border="0">

<tr>
	<td valign="top" width="250"><img src="projects/cvpr18_chaoyang/img2.png" width="300"></td>
	<td valign="top" width="600">
	<h3>Learning Depth from Monocular Videos using Direct Methods</h3>
	<p><strong>Chaoyang Wang</strong>, Jose Miguel Buenaposada, Rui Zhu, Simon Lucey<br />
	<strong>[
	<a href="https://arxiv.org/pdf/1712.00175.pdf">arXiv:1712.00175</a>
	]
</strong><br />
Unsupervised strategies to learning a depth estimator are particularly
appealing as they can utilize much larger and varied monocular video datasets during
learning without the need for ground truth depth or stereo. In previous works,
separate pose and depth CNN predictors had to be determined such that
their joint outputs minimized the photometric error. Inspired by
recent advances in direct mehtods for visual SLAM, we argue that the depth CNN predictor can be learned without a pose CNN predictor.
Further, we demonstrate empirically that incorporation of a differentiable implementation of DVO,
along with a novel depth normalization strategy - substantially
 improves performance over state of the art that use monocular videos for training. </p>
	</td>
</tr> -->


<!-- <tr>
	<td width="250"><img src="projects/cvpr18_rui/img2.png" width="300"></td>
	<td valign="top" width="600">
	<h3>Semantic Photometric Bundle Adjustment on Natural Sequences</h3>
	<p>Rui Zhu, <strong>Chaoyang Wang</strong>, Ziyan Wang, Chen-Hsuan Lin, Simon Lucey<br />
	<strong>[
	<a href="https://arxiv.org/pdf/1712.00110.pdf">arXiv:1712.00110 </a>
	]</strong><br />
A fundamental drawback to (photometric) bundle adjustment
is: (i) their reliance on having to view all points on
the object, and (ii) for the object surface to be well textured.
To circumvent these limitations we propose semantic
PBA which incorporates a 3D object prior, obtained
through deep learning, within the photometric bundle adjustment
problem. We demonstrate state of the art performance
in comparison to leading methods for object reconstruction
across numerous natural sequences.
	</p>
	</td>
</tr>
</table> -->
<hr>


<h2>Selected Publications & Manuscripts</h2>
	<span><sup>*</sup>co-first author,</span>
	<span><sup>&#8224</sup>corresponding author</span>




	<table border="0">

		<tr>
			<td valign="middle" width="350">
				<video autoplay muted loop width="300">
					<source src="https://snap-research.github.io/4Real-Video/resources/real_world_anim_labeled/04-highres-dy-cat-24fps.mp4"  type="video/mp4">
				</video>
				<video autoplay muted loop width="300">
					<source src="https://snap-research.github.io/4Real-Video/resources/real_world_anim_labeled/04-highres-frz-cat-24fps.mp4"  type="video/mp4">
				</video>
			</td>
			<td valign="middle" width="600">
			<p>
			<papertitle>4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion</papertitle>
			<br>
			<b>Chaoyang Wang</b><sup>*</sup><sup>&#8224</sup>, Peiye Zhuang<sup>*</sup>, Tuan Duc Ngo<sup>*</sup>, Willi Menapace, Aliaksandr Siarohin, Michael Vasilkovsky, Ivan Skorokhodov, Sergey Tulyakov, Peter Wonka, Hsin-Ying Lee
			<br>
			in Submission
		<strong>[
			<a href="https://snap-research.github.io/4Real-Video/">project</a>
			]
		</strong>
		</p>
			</td>
		</tr>


		<tr>
			<td valign="middle" width="350">
				<center>
				<video autoplay muted loop width="230">
					<source src="https://snap-research.github.io/DELTA/resources/dense_videos/ours/birthcake.mp4"  type="video/mp4">
				</video>
				</center>
			</td>
			<td valign="middle" width="600">
			<p>
			<papertitle>DELTA: Dense Efficient Long-range 3D Tracking for Any Video</papertitle>
			<br>
			Tuan Duc Ngo, Peiye Zhuang, Chuang Gan, Evangelos Kalogerakis, Sergey Tulyakov, Hsin-Ying Lee, <b>Chaoyang Wang</b>
			<br>
			ICLR 2025
		<strong>[
			<a href="https://snap-research.github.io/DELTA/">project</a>
			]
		</strong>
		</p>
			</td>
		</tr>

		<tr>
			<td valign="middle" width="350">
				<!-- <div style="position: relative; width: 100%; overflow: hidden; height: 50%;"> -->
				<video autoplay muted loop width="300" src="projects/gtr/teaser.mp4" type="video/mp4">
				</div>
			</td>
			<td valign="middle" width="600">
			<p>
			<papertitle>GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement</papertitle>
			<br>
			Peiye Zhuang, Songfang Han,  <b>Chaoyang Wang</b>, Aliaksandr Siarohin, Jiaxu Zou, Michael Vasilkovsky,
			Vladislav Shakhrai, Sergey Korolev, Sergey Tulyakov, Hsin-Ying Lee
			<br>
			ICLR 2025
		<strong>[
			<a href="https://snap-research.github.io/GTR/">project</a>
			]
		</strong>
		</p>
			</td>
		</tr>


		<tr>
			<td valign="middle" width="350">
				<video autoplay muted loop width="300">
					<source src="https://snap-research.github.io/4Real/resources/teaser-1080p.mp4"  type="video/mp4">
				</video>
			</td>
			<td valign="middle" width="600">
			<p>
			<papertitle>4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models</papertitle>
			<br>
			Heng Yu<sup>*</sup>, <b>Chaoyang Wang</b><sup>*</sup>, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, László A. Jeni, Sergey Tulyakov, Hsin-Ying Lee
			<br>
			NeurIPS 2024
		<strong>[
			<a href="https://snap-research.github.io/4Real/">project</a>
			]
		</strong>
		</p>
			</td>
		</tr>

		<tr>
			<td valign="middle" width="350">
				<video autoplay muted loop width="150">
					<source src="https://zqh0253.github.io/SceneWiz3D/media/diverse/9.mp4"  type="video/mp4">
				</video>
				<video autoplay muted loop width="150">
					<source src="https://zqh0253.github.io/SceneWiz3D/media/artistic/9.mp4" type="video/mp4">
				</video>
			</td>
			<td valign="middle" width="600">
			<p>
			<papertitle>SceneWiz3D: Towards Text-guided 3D Scene Composition</papertitle>
			<br>
			Qihang Zhang, <b>Chaoyang Wang</b>, Aliaksandr Siarohin, Peiye Zhuang, Yinghao Xu, Ceyuan Yang, Dahua Lin, Bolei Zhou, Sergey Tulyakov, Hsin-Ying Lee
			<br>
			CVPR 2024
		<strong>[
			<a href="https://zqh0253.github.io/SceneWiz3D/">project</a>
			]
		</strong>
		</p>
			</td>
		</tr>


		<tr>
			<td valign="middle" width="350">
				<video autoplay muted loop width="300">
					<source src="projects/3dvader/teaser.mp4"  type="video/mp4">
				</video>
			</td>
			<td valign="middle" width="600">
			<p>
			<papertitle>3D VADER - AutoDecoding Latent 3D Diffusion Models</papertitle>
			<br>
			Evangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski, <b>Chaoyang Wang</b>, Luc Van Gool, Sergey Tulyakov
			<br>
			NeurIPS 2023
		<strong>[
			<a href="https://snap-research.github.io/3DVADER/">project</a>
			]
		</strong>
		</p>
			</td>
		</tr>

		<tr>
			<td valign="middle" width="350"><img src="projects/fsdnerf/resources/illu.png" width="300"></td>
			<td valign="middle" width="600">
			<p>
			<papertitle>Flow Supervison for Deformable NeRF</papertitle>
			<br>
			<b>Chaoyang Wang</b>, Lachlan E. MacDonald, Laszlo A. Jeni, Simon Lucey
			<br>
			CVPR 2023 (Highlight)
		<strong>[
			<a href="projects/fsdnerf/index.html">project</a>
			]
		</strong>
		</p>
			</td>
		</tr>
	
		<tr>
			<td valign="middle" width="350">
				<video autoplay muted loop width="300">
					<source src="https://gengshan-y.github.io/images/rac.mp4"  type="video/mp4">
				</video>
			</td>
			<td valign="middle" width="600">
			<p>
			<papertitle>RAC: Reconstructing Animatable Categories from Videos</papertitle>
			<br>
			Gengshan Yang, <b>Chaoyang Wang</b>, N dinesh reddy, Deva Ramanan
			<br>
			CVPR 2023
		<strong>[
			<a href="https://gengshan-y.github.io/rac-www/">project</a>
			]
		</strong>
		</p>
			</td>
		</tr>
	</table>

	<table border="0">

		<tr>
			<td valign="topleft" width="350">	
					<!-- <img src="projects/cvpr22/b1c_ntp.gif" width="150">	
					<img src="projects/cvpr22/6db_ntp.gif" width="150">	 -->
			<video width="300" autoplay loop muted>
				<source src="projects/ntp/balloon_teaser.mp4" type="video/mp4" />
			</video>
			</td>

			<td valign="top" width="600">
			<p>
				<papertitle>Neural Priors for Trajectory Estimation</papertitle>
				<br>
				<b>Chaoyang Wang</b>, Xueqian Li,  Jhony Kaesemodel Pontes, Simon Lucey
				<br>
				CVPR 2022
				<strong>[
					<a href="projects/cvpr22/supplementary/supp.html">supp</a>
					]
				</strong>
			</p>

			<p>
			<papertitle>Neural Trajectory Fields for Dynamic Novel View Synthesis</papertitle>
			<br>
			<b>Chaoyang Wang</b>, Ben Eckart, Simon Lucey, Orazio Gallo
			<br>
			 2021
		<strong>[
			<a href="https://arxiv.org/pdf/2105.05994.pdf">arxiv</a>
			]
		</strong>
		</p>
			</td>
		</tr>
	</table>

	<h2>Older Publications</h2>

<table border="0">

	<tr>
		<td valign="top" width="250"><img src="projects/cvpr21/paul_teaser.gif" width="200"></td>
		<td valign="top" width="600">
		<p>
		<papertitle>PAUL: Procrustean Autoencoder for Unsupervised Lifting</papertitle>
		<br>
		Chaoyang Wang, Simon Lucey
		<br>
		CVPR 2021
	<strong>[
		<a href="https://arxiv.org/pdf/2103.16773.pdf">arxiv</a>
		]
	</strong>
	</p>
		</td>
	</tr>

<!-- <table border="0">

	<tr>
		<td valign="top" width="250"><img src="" width="300"></td>
		<td valign="top" width="600">
		<p>
		<papertitle>High Fidelity 3D Reconstructions with Limited Physical Views</papertitle>
		<br>
		Mosam Dabhi, Chaoyang Wang, Kunal Saluja, Laszlo Jeni, Ian Fasel, Simon Lucey
		<br>
		3DV 2021
	<strong>[
		<a href="https://arxiv.org/pdf/2110.11599.pdf">arxiv</a>,
		<a href="https://sites.google.com/view/high-fidelity-3d-neural-prior">project</a>
		]
	</strong>
	</p>
		</td>
	</tr>
</table> -->

<table border="0">

	<tr>
		<td valign="top" width="250"><img src="projects/nips20/r11.png" width="200"></td>
		<td valign="top" width="600">
		<p>
		<papertitle>SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static Images</papertitle>
		<br>
		Chen-Hsuan Lin, Chaoyang Wang, Simon Lucey
		<br>
		NeurIPS 2020
		<strong>[
		<a href="https://arxiv.org/abs/2010.10505">arxiv</a>,
		<a href="https://chenhsuanlin.bitbucket.io/signed-distance-SRN">project</a>
		]
	</strong>
	</p>
	<!-- First, we demonstrate that there is a theoretical relationship between siamese regression networks like GOTURN and the classical Inverse Compositional Lucas & Kanade (IC-LK) algorithm.
	Further, we demonstrate that unlike GOTURN IC-LK adapts its regressor to the appearance of the current tracked frame.
	Second, we propose a novel framework for object tracking inspired by the IC-LK framework, which we refer to as Deep-LK.
	Finally, we show impressive results demonstrating that, with end-to-end learned features, Deep-LK substantially outperforms GOTURN and demonstrates comparable tracking performance against current state-of-the-art deep trackers on high frame-rate sequences whilst being an order of magnitude (100 FPS) computationally efficient.</p> -->
		</td>
	</tr>


<table border="0">
	<tr>
		<td valign="top" width="250"><img src="projects/3dv20/deepnrsfmpp.jpg" width="200"></td>
		<td valign="top" width="600">
		<p>
		<papertitle>Deep NRSfM++: Towards Unsupervised 2D-3D Lifting in the Wild</papertitle>
		<br>
		Chaoyang Wang, Chen-Hsuan Lin, Simon Lucey
		<br>
		3DV 2020 (oral)
		<strong>[
			<a href="https://arxiv.org/pdf/3001.10090.pdf">arxiv</a>
		]
	</strong>
	</p>
	<!-- First, we demonstrate that there is a theoretical relationship between siamese regression networks like GOTURN and the classical Inverse Compositional Lucas & Kanade (IC-LK) algorithm.
	Further, we demonstrate that unlike GOTURN IC-LK adapts its regressor to the appearance of the current tracked frame.
	Second, we propose a novel framework for object tracking inspired by the IC-LK framework, which we refer to as Deep-LK.
	Finally, we show impressive results demonstrating that, with end-to-end learned features, Deep-LK substantially outperforms GOTURN and demonstrates comparable tracking performance against current state-of-the-art deep trackers on high frame-rate sequences whilst being an order of magnitude (100 FPS) computationally efficient.</p> -->
		</td>
	</tr>

	<tr>
			<td valign="top" width="250"><img src="projects/iccv19/iccv19-teaser.png" width="200"></td>
			<td valign="top" width="600">
			<p>
			<papertitle>Distill Knowledge from NRSfM for Weakly Supervised 3D Pose Learning</papertitle>
			<br>
			Chaoyang Wang, Chen Kong, Simon Lucey
			<br>
			ICCV 2019
			<strong>[
			<a href="https://arxiv.org/pdf/1908.06377.pdf">arxiv</a>,
			<a href="https://mightychaos.github.io/"> project </a>
			]
		</strong>
		</p>
		<!-- First, we demonstrate that there is a theoretical relationship between siamese regression networks like GOTURN and the classical Inverse Compositional Lucas & Kanade (IC-LK) algorithm.
		Further, we demonstrate that unlike GOTURN IC-LK adapts its regressor to the appearance of the current tracked frame.
		Second, we propose a novel framework for object tracking inspired by the IC-LK framework, which we refer to as Deep-LK.
		Finally, we show impressive results demonstrating that, with end-to-end learned features, Deep-LK substantially outperforms GOTURN and demonstrates comparable tracking performance against current state-of-the-art deep trackers on high frame-rate sequences whilst being an order of magnitude (100 FPS) computationally efficient.</p> -->
			</td>
	</tr>

	<tr>
			<td valign="top" width="250"><img src="projects/wsvd/teaser.png" width="200"></td>
			<td valign="top" width="600">
			<p>
			<papertitle>Web Stereo Video Supervision for Depth Prediction from Dynamic Scenes</papertitle>
			<br>
			Chaoyang Wang, Simon Lucey, Federico Perazzi, Oliver Wang<br>
			3DV 2019
			<strong>[
			<a href="http://www.ci2cv.net/media/papers/3dv_webstereo.pdf">PDF</a>,
			<a href="https://sites.google.com/view/wsvd/">project</a>
			]
		</strong>
		</p>
		<!-- First, we demonstrate that there is a theoretical relationship between siamese regression networks like GOTURN and the classical Inverse Compositional Lucas & Kanade (IC-LK) algorithm.
		Further, we demonstrate that unlike GOTURN IC-LK adapts its regressor to the appearance of the current tracked frame.
		Second, we propose a novel framework for object tracking inspired by the IC-LK framework, which we refer to as Deep-LK.
		Finally, we show impressive results demonstrating that, with end-to-end learned features, Deep-LK substantially outperforms GOTURN and demonstrates comparable tracking performance against current state-of-the-art deep trackers on high frame-rate sequences whilst being an order of magnitude (100 FPS) computationally efficient.</p> -->
			</td>
	</tr>


	<tr>
		<td valign="top" width="250"><img src="projects/cvpr18_chaoyang/img2.png" width="200"></td>
		<td valign="top" width="600">
		<p>
		<papertitle>Learning Depth from Monocular Videos using Direct Methods</papertitle>
		<br>
		Chaoyang Wang, Jose Miguel Buenaposada, Rui Zhu, Simon Lucey<br>
			CVPR 2018
		<strong>[
		<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Learning_Depth_From_CVPR_2018_paper.pdf">PDF</a>,
	  <strong><a href="https://github.com/MightyChaos/LKVOLearner">project</a>
		]
		</p>
	<!-- </strong><br />
	<strong>Result on KITTI Eigen split: <a href="https://drive.google.com/open?id=1Wj7ulSimrvrzNx4TRd-JspmX3DJwgPiV">[posenet(K)]</a>
	<a href="https://drive.google.com/open?id=1wiODwgX_Vm_w7fVK1y_X5CNJTtgaPwcN">[ddvo(K)]</a>
	<a href="https://drive.google.com/open?id=1uUQJLcUOoY2hG6QS_F-wbM3GDAjD-Z5h">[posenet+ddvo(K)]</a>
	<a href="https://drive.google.com/open?id=1hp4zFgK5NSNGdvaQL2ZumeinMQY_-AwK">[posenet+ddvo(K+CS)]</a>
</strong><br /> -->
	<!-- Unsupervised strategies to learning a depth estimator are particularly
	appealing as they can utilize much larger and varied monocular video datasets during
	learning without the need for ground truth depth or stereo. In previous works,
	separate pose and depth CNN predictors had to be determined such that
	their joint outputs minimized the photometric error. Inspired by
	recent advances in direct mehtods for visual SLAM, we argue that the depth CNN predictor can be learned without a pose CNN predictor.
	Further, we demonstrate empirically that incorporation of a differentiable implementation of DVO,
	along with a novel depth normalization strategy - substantially
	 improves performance over state of the art that use monocular videos for training. </p> -->
		</td>
	</tr>

	<tr>
		<td valign="top" width="250"><img src="projects/accv18/teaser.jpg" width="200"></td>
		<td valign="top" width="600">
		<p>
		<papertitle>Deep Convolutional Compressed Sensing for LiDAR Depth Completion</papertitle>
		<br>
		Nathaniel Chodosh, Chaoyang Wang, Simon Lucey<br>
		ACCV 2018
		<strong>[
		<a href="http://www.ci2cv.net/media/papers/nate_accv_2018.pdf">PDF</a>,
		<a href="https://github.com/nchodosh/Super-LiDAR">code</a>
		]
	</strong>
	</p>
	<!-- First, we demonstrate that there is a theoretical relationship between siamese regression networks like GOTURN and the classical Inverse Compositional Lucas & Kanade (IC-LK) algorithm.
	Further, we demonstrate that unlike GOTURN IC-LK adapts its regressor to the appearance of the current tracked frame.
	Second, we propose a novel framework for object tracking inspired by the IC-LK framework, which we refer to as Deep-LK.
	Finally, we show impressive results demonstrating that, with end-to-end learned features, Deep-LK substantially outperforms GOTURN and demonstrates comparable tracking performance against current state-of-the-art deep trackers on high frame-rate sequences whilst being an order of magnitude (100 FPS) computationally efficient.</p> -->
		</td>
	</tr>



	<tr>
		<td valign="top" width="250"><img src="projects/icra18/img2.png" width="200"></td>
		<td valign="top" width="600">
		<p>
		<papertitle>Deep-LK for Efficient Adaptive Object Tracking</papertitle>
		<br>
		Chaoyang Wang, Hamed Kiani, Chen-Hsuan Lin, Simon Lucey<br />
		ICRA 2018
		<strong>[
		<a href="http://www.ci2cv.net/media/papers/deepLK-icra.pdf">PDF</a>,
		<a href="https://arxiv.org/pdf/1705.06839.pdf">arXiv</a>,
		<a href="https://youtu.be/1q3GrYf9k-E">demo</a>
		]
	</strong><br />
	<br>
	<!-- First, we demonstrate that there is a theoretical relationship between siamese regression networks like GOTURN and the classical Inverse Compositional Lucas & Kanade (IC-LK) algorithm.
	Further, we demonstrate that unlike GOTURN IC-LK adapts its regressor to the appearance of the current tracked frame.
	Second, we propose a novel framework for object tracking inspired by the IC-LK framework, which we refer to as Deep-LK.
	Finally, we show impressive results demonstrating that, with end-to-end learned features, Deep-LK substantially outperforms GOTURN and demonstrates comparable tracking performance against current state-of-the-art deep trackers on high frame-rate sequences whilst being an order of magnitude (100 FPS) computationally efficient.</p> -->
		</td>
	</tr>

<!-- <tr>
	<td valign="top" width="250"><img src="" width="300"></td>
	<td valign="top" width="600">
	<p>
	<papertitle>Object-Centric Photometric Bundle Adjustment with Deep Shape Prior</papertitle>
	<br>
	Rui Zhu, Chaoyang Wang,  Chen-Hsuan Lin, Simon Lucey<br />
	WACV 2018
	<strong>[
	<a href="https://arxiv.org/abs/1711.01470">PDF,  </a>
		<a href="https://arxiv.org/pdf/1712.00110.pdf"> Extended version</a>
	]
	</strong>
	</td>
	</p>
</tr> -->
<!--cur project1-->
<tr>
	<td valign="top" width="250"><img src="projects/iccv17/iccv17.png" width="200"></td>
	<td valign="top" width="600">
	<p>
	<papertitle>Rethinking Reprojection: Closing the Loop for Pose-aware Shape Reconstruction from a Single Image</papertitle>
	<br>
	Rui Zhu, Hamed Kiani, Chaoyang Wang, Simon Lucey<br />
	ICCV 2017
	<strong>[
	<a href="https://arxiv.org/abs/1707.04682">PDF</a>
	]
	</strong>
	</p>
	</td>
</tr>

<!--cur project1-->
<tr>
	<td valign="top" width="250"><img src="" width="200"></td>
	<td valign="top" width="600">
	<p>
	<papertitle>Object Proposal by Multi-branched Hierarchical Segmentation</papertitle>
	<br>
	Chaoyang Wang, Long Zhao, Shuang Liang, Liqing Zhang, Jinyuan Jia, Yichen Wei<br />
	CVPR 2015
	<strong>[
	<a href="projects/objprop/Wang_Object_Proposal_by_2015_CVPR_paper.pdf">PDF</a>,
	<a href="http://pan.baidu.com/s/1oLDQe">VOC3007 test result</a>
	]
	</strong></p>
	</td>
</tr>
<!--project 2 -->
<tr>
	<td valign="top" width="250"><img src="projects/bpstereo/img1.png" width="200"></td>
	<td valign="top" width="600">
	<p>
	<papertitle>Binocular Photometric Stereo Acquisition and Reconstruction for 3D Talking Head Applications</papertitle>
	<br>
	Chaoyang Wang, Lijuan Wang, Yasuyuki Matsushita, Bojun Huang, Magnetro Chen, Frank K. Soong<br /> 
	INTERSPEECH 2013
	<strong>[
	<a href="projects/bpstereo/IS130859.PDF">PDF</a>,
	<a href="http://research.microsoft.com/en-us/projects/hd_talking_head/">web</a>,
	<a href="projects/bpstereo/demo.avi">demo</a>,
	<a href="projects/bpstereo/bpstereo.zip">code</a>
	]
	</strong></p>
	</td>
</tr>

</table>
<!--
<hr>
<h2>Course Projects</h2>
<table>
<tr><h3>Multi-task Asynchronous Deep Reinforcement Learning</h3></tr>
<tr>
	<td width="250"><img src="projects/703/img.png" width="300"></td>
	<td valign="top" width="600"> <p>We propose a computationally efficient multi-task
asynchronous learning framework, which requires only multi-threading on CPU
for training, generalizable to a wide variety of RL methods, and in theory, scales
well to the number of tasks. We test the proposed method with Atari games.
Through the experiment, we not only show that asynchronous multi-task training
is able to compress the policy and value networks for the agent through aggressive
parameter sharing, but also observe significant performance gain on several
games.[<a href="projects/703/RL_project.pdf">PDF</a>]</p></td>
</tr>
</table>
<table>
<tr><h3>Wasserstein Training of Deep Boltzmann Machines</h3></tr>
<tr>
	<td width="250"><img src="projects/703/img.png" width="300"></td>
	<td valign="top" width="850"> <p>We proposed a general Wasserstein training method for graphical models by replacing the standard KL-divergence with Wasserstein distance as novel loss functions.
		And we developed Wasserstein training of
RBMs and DBMs as specific examples. Finally,
we experimentally explored Wasserstein training
of RBMs and DBMs for digit generation with
MNIST dataset, and showed the superiority of
Wasserstein training compared to traditional KLdivergence
training.[<a href="projects/708/pgm_project_final.pdf">PDF</a>]</p> </td>
</tr>
</table>

<hr>
<h2>Software</h2>
<table border="0">
<ul>
	<li><strong>Learning Cost-Sensitive Decsion Policy</strong><br />
	Optimize decision thresholds for cascaded detectors.
	<strong>(
	<a href="https://wcy@bitbucket.org/wcy/learnpolicy.git">code</a>
	)</li>
<ul/>
</table> -->

<hr>
<i>Last modified by Wang Chaoyang, 2025-01-12.</i>
<div style="display:none"><a href='ri_thesis_poposal.pdf'><!-- nothing --></a></div>
<!-- <hr>
<table border="0">
<tr>
<a href="https://clustrmaps.com/site/19x5b" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=Rv_bWFfPipA9gtm9uwY6xRQGha_raL_Dx6yCs0fa4jY&cl=ffffff"></a>
</tr>
</table> -->

</body>
</center>
</html>
