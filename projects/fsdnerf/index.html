<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link rel="stylesheet" type="text/css" href="./styles.css" />

<html>
<head>
	<title>Flow Supervision for Deformable NeRF</title>
	<meta property="og:image" content="resources/illu.png/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Flow Supervision for Deformable NeRF." />
	<meta property="og:description" content="This paper discuss how to estimate 3D scene flows from the deformation field used in Deformable NeRF. We derived an analytical equation to calculate the flows without inverting the deformation field. We show applying flow supervion is helpful to improve reconstruction for fast moving objects." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Flow Supervision for Deformable NeRF</span>
        <br>
        <!-- <span style="font-size:36px">Supplementary Material</span> -->
		<table align=center width=700px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px">Chaoyang Wang</span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px">Lachlan Ewen MacDonald</span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px">Laszlo A. Jeni</span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px">Simon Lucey</span>
						</center>
					</td>
				</tr>
		</table>
		<table align=center width=400px>
			<tr>
				<td align=center width=100px>
					<a href="https://arxiv.org/pdf/2303.16333.pdf"><img src="icons/arxiv.jpeg" style='width:80px;'/></a>
				</td>
				<td align=center width=100px>
					<a href=""><img src="icons/GitHub-logo.png" alt="Original image" style="width:80px; opacity: 0.2;"/></a>
				</td>
			</tr>
		</table>
		
	</center>



    <br>
	<hr>
    <br>

    Deformable neural radiance fields (i.e. Nerfies[<a href=#cite_nerfies id='ref_nerfies'>2</a>] , HyperNeRF[<a href=#cite_hypernerf id='ref_hypernerf'>3</a>] ) has been a notable technique to represent dynamic scenes and shows plausible space-time view synthesis results. However, the current implementations only warrant success on teleporting-like
    videos whose camera motions are significantly more rapid than object motions. Quality of their results significantly decrease on videos with more rapid object motions [<a href=#cite_dycheck id='ref_dycheck'>4</a>]. For example, in the "broom" example below, both Nefies and HyperNeRF show significant artifacts when trained only using a single camera (i.e. the left camera from the stereo camera rig), as oppose to teleporting between the left and right cameras in the original paper of Nerfies.

	<table align=center width=900px>
		<center><h1>View synthesis result on Nerfies dataset [<a href=#cite_nerfies id='ref_nerfies'>2</a>] </h1></center>
        <tr>
            <center>
            <div style='width:90%; text-align:justify; float:center'>

            </center>
		</tr>
        <br>
        <tr>
            <th style='font-size: 22px'>Ours</th>
            <th style='font-size: 22px'>Nerfies [<a href=#cite_nerfies id='ref_nerfies'>2</a>]</th>
            <th style='font-size: 22px'>HyperNeRF [<a href=#cite_hypernerf id='ref_hypernerf'>3</a>]</th>
            <th style='font-size: 22px'>NSFF [<a href=#cite_nsff id='ref_nsff'>4</a>]</th>
        </tr>

		<tr>
			<td>
                <center>
				    <img class='round' style='width:150px' src='./resources/ours_broom.gif'>
                </center>
			</td>
			<td>
                <center>
				    <img class='round' style='width:150px' src='./resources/nerfies_broom.gif'>
                </center>
			</td>
			<td>
                <center>
				    <img class='round' style='width:150px' src='./resources/hypernerf_broom.gif'>
                </center>
			</td>
			<td>
                <center>
				    <img class='round' style='width:150px' src='./resources/nsff_broom.gif'>
                </center>
			</td>
		</tr>
    </table>

    <table align=center width=900px>
        <tr>
            <center>
            <div style='width:90%; text-align:justify; float:center'>
                In this work, we conjecture the deficiency of these deformable NeRF-based methods is mainly due to lack of temporal regularization. The community has explored optical flow as an additional cue to help supervise the temporal transitions of other motion representations, such as scene flow fields [<a href=#cite_nsff id='ref_nsff'>4</a>]  and blend skinning fields. However, enforcing flow constraints with respect to a generic backward deformation field as in Nerfies is non-trivial. We propose an elegant solution to enforce flow constraints for deformable NeRFs, without inverting the backward deformation function.
                Our method produces plausible view synthesis result, as well as accurate depth map and motions.
            </center>
        </tr>
        <br>
        <tr>
            <th style='font-size: 22px'>novel view (right cam.)</th>
            <th style='font-size: 22px'>depth</th>
            <th style='font-size: 22px'>deformation</th>
        </tr>
		<tr>
			<td>
                <center>
				    <img class='round' style='width:150px' src='./resources/ours_val.gif'>
                </center>
			</td>
			<td>
                <center>
				    <img class='round' style='width:150px' src='./resources/ours_depth_val.gif'>
                </center>
			</td>
			<td>
                <center>
				    <img class='round' style='width:150px' src='./resources/ours_dist_val.gif'>
                </center>
			</td>
		</tr>

    </table>

	<table align=center width=900px>
		<center><h1>View synthesis result on Nvidia dynamic view sythesis benchmark (NDVS) [<a href=#cite_nv id='ref_nv'>1</a>] </h1></center>
        <tr>
            <center>
            <div style='width:90%; text-align:justify; float:center'>
				We provide a side-by-side comparison between with and without optical flow supervision. Our implementation is based on Nerfies but use normalized device coordinate to deal with unbounded scenes in the NDVS dataset. 
            Our method with flow supervision significantly outperforms the version without flow supervision. 
            </center>
		</tr>
        <br>
        <tr>
            <th style='font-size: 22px'>with flow (Ours)</th>
            <th style='font-size: 22px'>w/o flow (*Nefies[<a href=#cite_nerfies id='ref_nerfies'>2</a>])</th>
        </tr>
		<tr>
			<td>
                <center>
				    <img class='round' style='width:350px' src='./resources/with_flow/balloon/rgb.gif'>
                </center>
			</td>
            <td>
                <center>
				    <img class='round' style='width:350px' src='./resources/wo_flow/balloon/rgb.gif'>
                </center>
			</td>
		</tr>
        <tr>
			<td>
                <center>
				    <img class='round' style='width:350px' src='./resources/with_flow/balloon/depth.gif'>
                </center>
			</td>
            <td>
                <center>
				    <img class='round' style='width:350px' src='./resources/wo_flow/balloon/depth.gif'>
                </center>
			</td>
		</tr>
        <tr>
			<td>
                <center>
				    <img class='round' style='width:350px' src='./resources/with_flow/balloon/dist.gif'>
                </center>
			</td>
            <td>
                <center>
				    <img class='round' style='width:350px' src='./resources/wo_flow/balloon/dist.gif'>
                </center>
			</td>
		</tr>

        <br>
        <tr>
            <th style='font-size: 22px'>with flow (Ours)</th>
            <th style='font-size: 22px'>w/o flow (*Nefies[<a href=#cite_nerfies id='ref_nerfies'>2</a>])</th>
        </tr>
		<tr>
			<td>
                <center>
				    <img class='round' style='width:350px' src='./resources/with_flow/playground/rgb.gif'>
                </center>
			</td>
            <td>
                <center>
				    <img class='round' style='width:350px' src='./resources/wo_flow/playground/rgb.gif'>
                </center>
			</td>
		</tr>
        <tr>
			<td>
                <center>
				    <img class='round' style='width:350px' src='./resources/with_flow/playground/depth.gif'>
                </center>
			</td>
            <td>
                <center>
				    <img class='round' style='width:350px' src='./resources/wo_flow/playground/depth.gif'>
                </center>
			</td>
		</tr>
        <tr>
			<td>
                <center>
				    <img class='round' style='width:350px' src='./resources/with_flow/playground/dist.gif'>
                </center>
			</td>
            <td>
                <center>
				    <img class='round' style='width:350px' src='./resources/wo_flow/playground/dist.gif'>
                </center>
			</td>
		</tr>
	</table>
	<br>

	<hr>
	<table align=center width=850px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Reference</h1></center>
                    <div>[<a href=#ref_nv id='cite_nv'>1</a>] Yoon, Jae Shin, et al. "Novel view synthesis of dynamic scenes with globally coherent depths from a monocular camera." CVPR 2020.</div>
					<div>[<a href=#ref_nerfies id='cite_nerfies'>2</a>] Park, Keunhong, et al. "Nerfies: Deformable neural radiance fields." CVPR 2021.</div>
                    <div>[<a href=#ref_hypernerf id='cite_hypernerf'>3</a>] Park Keunhong, et al. "Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields." SIGGRAPH 2021.</div>
                    <div>[<a href=#ref_nsff id='cite_nsff'>4</a>] Li, Zhengqi, et al. "Neural scene flow fields for space-time view synthesis of dynamic scenes." CVPR 2021.</div>
                    <div>[<a href=#ref_dycheck id='cite_dycheck'>5</a>] Gao, Hang, et al. "Monocular dynamic view synthesis: A reality check." NeurIPS 2022.</div>
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

